{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 7083090,
     "sourceType": "datasetVersion",
     "datasetId": 4080656
    }
   ],
   "dockerImageVersionId": 30587,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install IPython\n",
    "!pip install torch\n",
    "!pip install nltk"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-29T14:50:55.323667Z",
     "iopub.execute_input": "2023-11-29T14:50:55.324125Z",
     "iopub.status.idle": "2023-11-29T14:52:54.677696Z",
     "shell.execute_reply.started": "2023-11-29T14:50:55.324091Z",
     "shell.execute_reply": "2023-11-29T14:52:54.676056Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T22:38:30.068544200Z",
     "start_time": "2023-11-30T22:38:07.317638500Z"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from scipy) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from seaborn) (1.26.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from seaborn) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.45.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (4.45.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (0.0.post11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IPython in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (8.17.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (5.13.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from IPython) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from jedi>=0.16->IPython) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from stack-data->IPython) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from stack-data->IPython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from stack-data->IPython) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->IPython) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikita\\pycharmprojects\\nlp_movies\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "id": "eb69633ba523c634"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:52:54.680788Z",
     "iopub.execute_input": "2023-11-29T14:52:54.681792Z",
     "iopub.status.idle": "2023-11-29T14:52:58.246221Z",
     "shell.execute_reply.started": "2023-11-29T14:52:54.681738Z",
     "shell.execute_reply": "2023-11-29T14:52:58.244934Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T22:38:30.131506100Z",
     "start_time": "2023-11-30T22:38:30.084201500Z"
    }
   },
   "execution_count": 17,
   "outputs": [],
   "id": "efa4ac8da8346404"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "\n",
    "# Splitting the file into lines and then into fields\n",
    "lines = open(\"train_data.txt\", encoding=\"UTF-8\").readlines()\n",
    "for line in lines:\n",
    "    if line.strip():  # Checking if the line is not empty\n",
    "        fields = line.split(' ::: ')\n",
    "        if len(fields) == 4:  # Ensuring that there are exactly 4 fields\n",
    "            record = {\n",
    "                'Title': fields[1],\n",
    "                'Genre': fields[2],\n",
    "                'Description': fields[3]\n",
    "            }\n",
    "            data.append(record)\n",
    "\n",
    "# Converting the list of dictionaries to a DataFrame for better visualization\n",
    "df = pd.DataFrame(data)\n",
    "df.head()  # Displaying the first few records to verify the parsing\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:52:58.248522Z",
     "iopub.execute_input": "2023-11-29T14:52:58.249269Z",
     "iopub.status.idle": "2023-11-29T14:52:59.102519Z",
     "shell.execute_reply.started": "2023-11-29T14:52:58.249224Z",
     "shell.execute_reply": "2023-11-29T14:52:59.101276Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T22:38:31.106667700Z",
     "start_time": "2023-11-30T22:38:30.405076Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                              Title     Genre  \\\n0      Oscar et la dame rose (2009)     drama   \n1                      Cupid (1997)  thriller   \n2  Young, Wild and Wonderful (1980)     adult   \n3             The Secret Sin (1915)     drama   \n4            The Unrecovered (2007)     drama   \n\n                                         Description  \n0  Listening in to a conversation between his doc...  \n1  A brother and sister with a past incestuous re...  \n2  As the bus empties the students for their fiel...  \n3  To help their unemployed father make ends meet...  \n4  The film's title refers not only to the un-rec...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Genre</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Oscar et la dame rose (2009)</td>\n      <td>drama</td>\n      <td>Listening in to a conversation between his doc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cupid (1997)</td>\n      <td>thriller</td>\n      <td>A brother and sister with a past incestuous re...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Young, Wild and Wonderful (1980)</td>\n      <td>adult</td>\n      <td>As the bus empties the students for their fiel...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The Secret Sin (1915)</td>\n      <td>drama</td>\n      <td>To help their unemployed father make ends meet...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Unrecovered (2007)</td>\n      <td>drama</td>\n      <td>The film's title refers not only to the un-rec...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "4f6fa00262537192"
  },
  {
   "cell_type": "code",
   "source": [
    "df1 = df[['Description', 'Genre']].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:52:59.105586Z",
     "iopub.execute_input": "2023-11-29T14:52:59.106080Z",
     "iopub.status.idle": "2023-11-29T14:52:59.131401Z",
     "shell.execute_reply.started": "2023-11-29T14:52:59.106048Z",
     "shell.execute_reply": "2023-11-29T14:52:59.129971Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T22:38:32.560948900Z",
     "start_time": "2023-11-30T22:38:31.106667700Z"
    }
   },
   "execution_count": 19,
   "outputs": [],
   "id": "ed30a292631ee688"
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK resources\n",
    "nltk.download('punkt')       # For tokenization\n",
    "nltk.download('wordnet')     # For lemmatization\n",
    "nltk.download('stopwords')   # For stop words\n",
    "\n",
    "# Now you can proceed with your text processing functions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercasing and removing punctuation\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing stop words and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply the cleaning function to your DataFrame\n",
    "df1['cleaned_description'] = df1['Description'].apply(clean_text)\n",
    "\n",
    "# Example of cleaned text\n",
    "print(df1['cleaned_description'].head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:57:10.669245Z",
     "iopub.execute_input": "2023-11-29T14:57:10.669808Z",
     "iopub.status.idle": "2023-11-29T14:57:11.670162Z",
     "shell.execute_reply.started": "2023-11-29T14:57:10.669773Z",
     "shell.execute_reply": "2023-11-29T14:57:11.668195Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T22:39:01.650971300Z",
     "start_time": "2023-11-30T22:38:32.560948900Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [listening, conversation, doctor, parent, 10ye...\n",
      "1    [brother, sister, past, incestuous, relationsh...\n",
      "2    [bus, empty, student, field, trip, museum, nat...\n",
      "3    [help, unemployed, father, make, end, meet, ed...\n",
      "4    [film, title, refers, unrecovered, body, groun...\n",
      "Name: cleaned_description, dtype: object\n"
     ]
    }
   ],
   "id": "62f7582522ef0615"
  },
  {
   "cell_type": "code",
   "source": [
    "texts = df1['cleaned_description'].tolist()\n",
    "labels = df1['Genre'].tolist()\n",
    "\n",
    "# Convert labels to numeric form if they are not already\n",
    "label_to_ix = {label: i for i, label in enumerate(set(labels))}\n",
    "numeric_labels = [label_to_ix[label] for label in labels]\n",
    "\n",
    "# Tokenize texts\n",
    "tokenized_texts = texts\n",
    "\n",
    "# Build a vocabulary\n",
    "vocab = Counter(word for sentence in tokenized_texts for word in sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-29T14:53:01.918658Z",
     "iopub.status.idle": "2023-11-29T14:53:01.920248Z",
     "shell.execute_reply.started": "2023-11-29T14:53:01.919965Z",
     "shell.execute_reply": "2023-11-29T14:53:01.919995Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T22:39:02.364723100Z",
     "start_time": "2023-11-30T22:39:01.667003900Z"
    }
   },
   "execution_count": 21,
   "outputs": [],
   "id": "68fe6acea813a4ef"
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "NUM_CLASSES = len(label_to_ix)  # Number of unique labels\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_to_ix):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word_to_ix = word_to_ix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return [self.word_to_ix[word] for word in text], label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    lengths = [len(text) for text in texts]\n",
    "    padded_texts = torch.zeros(len(texts), max(lengths)).long()\n",
    "    for i, text in enumerate(texts):\n",
    "        padded_texts[i, :lengths[i]] = torch.LongTensor(text)\n",
    "    return padded_texts, torch.tensor(labels)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    tokenized_texts, numeric_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels, word_to_ix)\n",
    "test_dataset = TextDataset(test_texts, test_labels, word_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:53:01.921385Z",
     "iopub.status.idle": "2023-11-29T14:53:01.922069Z",
     "shell.execute_reply.started": "2023-11-29T14:53:01.921841Z",
     "shell.execute_reply": "2023-11-29T14:53:01.921863Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T22:39:02.774813300Z",
     "start_time": "2023-11-30T22:39:02.367723100Z"
    }
   },
   "execution_count": 22,
   "outputs": [],
   "id": "3d71f7c1429c344b"
  },
  {
   "cell_type": "code",
   "source": [
    "class CBOWClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_classes):\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, num_classes)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embeds = self.embeddings(text)\n",
    "        embeds_mean = torch.mean(embeds, dim=1)\n",
    "        out = self.linear(embeds_mean)\n",
    "        return out\n",
    "\n",
    "model = CBOWClassifier(VOCAB_SIZE, EMBEDDING_DIM, NUM_CLASSES)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:53:01.923817Z",
     "iopub.status.idle": "2023-11-29T14:53:01.924683Z",
     "shell.execute_reply.started": "2023-11-29T14:53:01.924380Z",
     "shell.execute_reply": "2023-11-29T14:53:01.924405Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T22:39:02.948025200Z",
     "start_time": "2023-11-30T22:39:02.782827900Z"
    }
   },
   "execution_count": 23,
   "outputs": [],
   "id": "9b692d6a1d2421cf"
  },
  {
   "cell_type": "code",
   "source": [
    "EPOCHS = 100  # Adjust as needed\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for context, targets in data_loader:\n",
    "            outputs = model(context)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for context, target in train_loader:\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    accuracies = [calculate_accuracy(model, test_loader) for _ in range(10)]\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_deviation = np.std(accuracies)\n",
    "    print(f\"Epoch {epoch}: Total Loss: {total_loss}\")\n",
    "    print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "    print(f\"Standard Deviation: {std_deviation}\")"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:53:01.925994Z",
     "iopub.status.idle": "2023-11-29T14:53:01.927188Z",
     "shell.execute_reply.started": "2023-11-29T14:53:01.926764Z",
     "shell.execute_reply": "2023-11-29T14:53:01.926810Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:55:13.322520600Z",
     "start_time": "2023-12-01T17:09:56.349205800Z"
    }
   },
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total Loss: 25750.02274154636\n",
      "Mean Accuracy: 0.5466107165913493\n",
      "Standard Deviation: 0.0007598947544079778\n",
      "Epoch 1: Total Loss: 25712.236019862234\n",
      "Mean Accuracy: 0.5442589689200406\n",
      "Standard Deviation: 0.0013878007419331749\n",
      "Epoch 2: Total Loss: 25603.445429834523\n",
      "Mean Accuracy: 0.5475421931199853\n",
      "Standard Deviation: 0.0013753648903297596\n",
      "Epoch 3: Total Loss: 25522.788844910392\n",
      "Mean Accuracy: 0.5435211657290416\n",
      "Standard Deviation: 0.0014641490083899971\n",
      "Epoch 4: Total Loss: 25441.90652209305\n",
      "Mean Accuracy: 0.5405976205847091\n",
      "Standard Deviation: 0.0015629688431955293\n",
      "Epoch 5: Total Loss: 25484.65677578526\n",
      "Mean Accuracy: 0.5422207876049063\n",
      "Standard Deviation: 0.0007804361522840418\n",
      "Epoch 6: Total Loss: 25324.33948411724\n",
      "Mean Accuracy: 0.5442313013003781\n",
      "Standard Deviation: 0.001195697939439069\n",
      "Epoch 7: Total Loss: 25312.17762055178\n",
      "Mean Accuracy: 0.535977128101079\n",
      "Standard Deviation: 0.0019716401241815507\n",
      "Epoch 8: Total Loss: 25209.79203722952\n",
      "Mean Accuracy: 0.5460389191183251\n",
      "Standard Deviation: 0.0013250000942006696\n",
      "Epoch 9: Total Loss: 25224.803038693743\n",
      "Mean Accuracy: 0.5342156229825694\n",
      "Standard Deviation: 0.0017175974697076978\n",
      "Epoch 10: Total Loss: 25084.023330054748\n",
      "Mean Accuracy: 0.5460758092778752\n",
      "Standard Deviation: 0.0012265978050355181\n",
      "Epoch 11: Total Loss: 25081.130196617043\n",
      "Mean Accuracy: 0.5382550954532878\n",
      "Standard Deviation: 0.002306759847334862\n",
      "Epoch 12: Total Loss: 24985.900791996042\n",
      "Mean Accuracy: 0.5418242183897445\n",
      "Standard Deviation: 0.0015764879227403172\n",
      "Epoch 13: Total Loss: 24913.34443408408\n",
      "Mean Accuracy: 0.5455501245042885\n",
      "Standard Deviation: 0.0017071417079652026\n",
      "Epoch 14: Total Loss: 24842.546631308127\n",
      "Mean Accuracy: 0.5448215438531772\n",
      "Standard Deviation: 0.0010427997260578968\n",
      "Epoch 15: Total Loss: 24831.912843024504\n",
      "Mean Accuracy: 0.5423037904638937\n",
      "Standard Deviation: 0.0010209980634952088\n",
      "Epoch 16: Total Loss: 24753.275134601863\n",
      "Mean Accuracy: 0.5439915152633035\n",
      "Standard Deviation: 0.0005908906568160572\n",
      "Epoch 17: Total Loss: 24626.527219590615\n",
      "Mean Accuracy: 0.5466199391312367\n",
      "Standard Deviation: 0.001295102070020617\n",
      "Epoch 18: Total Loss: 24511.83978854271\n",
      "Mean Accuracy: 0.5421470072858066\n",
      "Standard Deviation: 0.0015472167868212968\n",
      "Epoch 19: Total Loss: 24580.990688037127\n",
      "Mean Accuracy: 0.542423683482431\n",
      "Standard Deviation: 0.0010921995095755025\n",
      "Epoch 20: Total Loss: 24460.595053131052\n",
      "Mean Accuracy: 0.5427925850779305\n",
      "Standard Deviation: 0.0014348387128995212\n",
      "Epoch 21: Total Loss: 24462.444701214103\n",
      "Mean Accuracy: 0.5453564511666513\n",
      "Standard Deviation: 0.0013352314133004148\n",
      "Epoch 22: Total Loss: 24352.825111430488\n",
      "Mean Accuracy: 0.5426542469796182\n",
      "Standard Deviation: 0.0009539869439074636\n",
      "Epoch 23: Total Loss: 24330.82402142766\n",
      "Mean Accuracy: 0.5429770358756801\n",
      "Standard Deviation: 0.0016504224078366851\n",
      "Epoch 24: Total Loss: 24279.863982395527\n",
      "Mean Accuracy: 0.5466383842110117\n",
      "Standard Deviation: 0.0010957371015607497\n",
      "Epoch 25: Total Loss: 24132.262824845388\n",
      "Mean Accuracy: 0.5428755879369177\n",
      "Standard Deviation: 0.001764954158650655\n",
      "Epoch 26: Total Loss: 24061.65609741027\n",
      "Mean Accuracy: 0.536945494789265\n",
      "Standard Deviation: 0.000809747405303\n",
      "Epoch 27: Total Loss: 24001.36167684215\n",
      "Mean Accuracy: 0.5401272710504473\n",
      "Standard Deviation: 0.00139086175571402\n",
      "Epoch 28: Total Loss: 23994.24728337361\n",
      "Mean Accuracy: 0.5448584340127272\n",
      "Standard Deviation: 0.0011978300701257325\n",
      "Epoch 29: Total Loss: 23943.214542021844\n",
      "Mean Accuracy: 0.5453656737065388\n",
      "Standard Deviation: 0.001194096343051875\n",
      "Epoch 30: Total Loss: 23887.95614020837\n",
      "Mean Accuracy: 0.5469703956469611\n",
      "Standard Deviation: 0.0012467837031169362\n",
      "Epoch 31: Total Loss: 23790.396418663\n",
      "Mean Accuracy: 0.5405238402656091\n",
      "Standard Deviation: 0.0010089728396809202\n",
      "Epoch 32: Total Loss: 23650.43332106358\n",
      "Mean Accuracy: 0.5452826708475513\n",
      "Standard Deviation: 0.0013024695010695354\n",
      "Epoch 33: Total Loss: 23693.66359976551\n",
      "Mean Accuracy: 0.5418518860094069\n",
      "Standard Deviation: 0.0016809581177688673\n",
      "Epoch 34: Total Loss: 23597.416326249528\n",
      "Mean Accuracy: 0.5439269574840911\n",
      "Standard Deviation: 0.0015458693601222582\n",
      "Epoch 35: Total Loss: 23583.461315032997\n",
      "Mean Accuracy: 0.5461034768975376\n",
      "Standard Deviation: 0.001220863653481358\n",
      "Epoch 36: Total Loss: 23483.243929400414\n",
      "Mean Accuracy: 0.5424052384026561\n",
      "Standard Deviation: 0.0013778672128154793\n",
      "Epoch 37: Total Loss: 23414.145893832785\n",
      "Mean Accuracy: 0.5423314580835562\n",
      "Standard Deviation: 0.0013264115802716846\n",
      "Epoch 38: Total Loss: 23297.5381052136\n",
      "Mean Accuracy: 0.5453380060868763\n",
      "Standard Deviation: 0.0009971011600938024\n",
      "Epoch 39: Total Loss: 23298.46779084386\n",
      "Mean Accuracy: 0.5421008945863691\n",
      "Standard Deviation: 0.0007933528790042076\n",
      "Epoch 40: Total Loss: 23261.07902144268\n",
      "Mean Accuracy: 0.5441298533616158\n",
      "Standard Deviation: 0.0012489648265041205\n",
      "Epoch 41: Total Loss: 23161.929125231167\n",
      "Mean Accuracy: 0.543069261274555\n",
      "Standard Deviation: 0.001137218076821195\n",
      "Epoch 42: Total Loss: 23139.519736222806\n",
      "Mean Accuracy: 0.5478926496357096\n",
      "Standard Deviation: 0.0010761562036937906\n",
      "Epoch 43: Total Loss: 23047.50501540792\n",
      "Mean Accuracy: 0.5470995112053859\n",
      "Standard Deviation: 0.0009914123139630427\n",
      "Epoch 44: Total Loss: 22971.996146216574\n",
      "Mean Accuracy: 0.5441021857419533\n",
      "Standard Deviation: 0.0013529500591562523\n",
      "Epoch 45: Total Loss: 22904.753185573936\n",
      "Mean Accuracy: 0.5452642257677764\n",
      "Standard Deviation: 0.0010237019275108133\n",
      "Epoch 46: Total Loss: 22882.47563133412\n",
      "Mean Accuracy: 0.5438162870054415\n",
      "Standard Deviation: 0.0016447178833504054\n",
      "Epoch 47: Total Loss: 22857.55225633744\n",
      "Mean Accuracy: 0.5402102739094347\n",
      "Standard Deviation: 0.001924065853087257\n",
      "Epoch 48: Total Loss: 22707.316975719557\n",
      "Mean Accuracy: 0.5419256663285069\n",
      "Standard Deviation: 0.0017837689163929264\n",
      "Epoch 49: Total Loss: 22721.197214307984\n",
      "Mean Accuracy: 0.5436041685880292\n",
      "Standard Deviation: 0.0007869480394721582\n",
      "Epoch 50: Total Loss: 22593.359406628493\n",
      "Mean Accuracy: 0.5383934335516001\n",
      "Standard Deviation: 0.0010562123620562753\n",
      "Epoch 51: Total Loss: 22575.700410215708\n",
      "Mean Accuracy: 0.5402471640689845\n",
      "Standard Deviation: 0.0016977741591074956\n",
      "Epoch 52: Total Loss: 22527.20127636941\n",
      "Mean Accuracy: 0.5450889975099142\n",
      "Standard Deviation: 0.0016133910330236709\n",
      "Epoch 53: Total Loss: 22463.18216050511\n",
      "Mean Accuracy: 0.5432075993728673\n",
      "Standard Deviation: 0.0017826003029935919\n",
      "Epoch 54: Total Loss: 22350.198493361677\n",
      "Mean Accuracy: 0.5469150604076363\n",
      "Standard Deviation: 0.001197368430664379\n",
      "Epoch 55: Total Loss: 22298.349627549163\n",
      "Mean Accuracy: 0.5428571428571429\n",
      "Standard Deviation: 0.0016486434764961\n",
      "Epoch 56: Total Loss: 22259.633562254312\n",
      "Mean Accuracy: 0.5416766577515448\n",
      "Standard Deviation: 0.0012187718186245017\n",
      "Epoch 57: Total Loss: 22182.189013284333\n",
      "Mean Accuracy: 0.5393156875403486\n",
      "Standard Deviation: 0.0012323742940876224\n",
      "Epoch 58: Total Loss: 22136.194638535762\n",
      "Mean Accuracy: 0.5362814719173661\n",
      "Standard Deviation: 0.0019183991412296097\n",
      "Epoch 59: Total Loss: 22073.106108721047\n",
      "Mean Accuracy: 0.5328230194595591\n",
      "Standard Deviation: 0.0012082583222475201\n",
      "Epoch 60: Total Loss: 22073.324810068996\n",
      "Mean Accuracy: 0.5378308586184635\n",
      "Standard Deviation: 0.001727374581197991\n",
      "Epoch 61: Total Loss: 21949.04065629882\n",
      "Mean Accuracy: 0.5401272710504472\n",
      "Standard Deviation: 0.0019271357206100286\n",
      "Epoch 62: Total Loss: 21807.086906029712\n",
      "Mean Accuracy: 0.5373328414645394\n",
      "Standard Deviation: 0.0021510672278546106\n",
      "Epoch 63: Total Loss: 21810.35837420362\n",
      "Mean Accuracy: 0.5445448676565526\n",
      "Standard Deviation: 0.00160124647364922\n",
      "Epoch 64: Total Loss: 21770.175295963294\n",
      "Mean Accuracy: 0.5426819145992805\n",
      "Standard Deviation: 0.002096602003379424\n",
      "Epoch 65: Total Loss: 21697.76981820012\n",
      "Mean Accuracy: 0.5386147745088997\n",
      "Standard Deviation: 0.002250299732546338\n",
      "Epoch 66: Total Loss: 21641.620355737716\n",
      "Mean Accuracy: 0.5419533339481694\n",
      "Standard Deviation: 0.001407276182621966\n",
      "Epoch 67: Total Loss: 21569.64442486805\n",
      "Mean Accuracy: 0.5399981554920225\n",
      "Standard Deviation: 0.0013684210636164616\n",
      "Epoch 68: Total Loss: 21486.63166156699\n",
      "Mean Accuracy: 0.5358387900027667\n",
      "Standard Deviation: 0.0014941937781407907\n",
      "Epoch 69: Total Loss: 21459.423749264686\n",
      "Mean Accuracy: 0.5375818500415015\n",
      "Standard Deviation: 0.0007131832492550549\n",
      "Epoch 70: Total Loss: 21389.239403958996\n",
      "Mean Accuracy: 0.543143041593655\n",
      "Standard Deviation: 0.0012564674075841446\n",
      "Epoch 71: Total Loss: 21289.365721868988\n",
      "Mean Accuracy: 0.540237941529097\n",
      "Standard Deviation: 0.0010513695471732918\n",
      "Epoch 72: Total Loss: 21313.99990289338\n",
      "Mean Accuracy: 0.5436779489071291\n",
      "Standard Deviation: 0.0016214106433115338\n",
      "Epoch 73: Total Loss: 21140.86099682079\n",
      "Mean Accuracy: 0.5385409941897998\n",
      "Standard Deviation: 0.0018325722834830476\n",
      "Epoch 74: Total Loss: 21171.488550095157\n",
      "Mean Accuracy: 0.5319561007101357\n",
      "Standard Deviation: 0.002686277485529949\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m log_probs \u001B[38;5;241m=\u001B[39m model(context)\n\u001B[0;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(log_probs, target)\n\u001B[1;32m---> 19\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     21\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\nlp_movies\\venv\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\nlp_movies\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "id": "a6128410376b8596"
  },
  {
   "cell_type": "code",
   "source": [
    "def get_predictions_and_targets(model, data_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for context, targets in data_loader:\n",
    "            outputs = model(context)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    return all_predictions, all_targets\n",
    "\n",
    "predictions, targets = get_predictions_and_targets(model, test_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:53:01.933183Z",
     "iopub.status.idle": "2023-11-29T14:53:01.933873Z",
     "shell.execute_reply.started": "2023-11-29T14:53:01.933560Z",
     "shell.execute_reply": "2023-11-29T14:53:01.933591Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:55:20.187080Z",
     "start_time": "2023-12-01T23:55:17.387966100Z"
    }
   },
   "execution_count": 38,
   "outputs": [],
   "id": "c43cb75b5efd3aa6"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(targets, predictions, target_names=label_to_ix.keys()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-29T14:53:01.936000Z",
     "iopub.status.idle": "2023-11-29T14:53:01.936658Z",
     "shell.execute_reply.started": "2023-11-29T14:53:01.936329Z",
     "shell.execute_reply": "2023-11-29T14:53:01.936357Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T23:55:23.452283800Z",
     "start_time": "2023-12-01T23:55:23.399813100Z"
    }
   },
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      horror       0.56      0.61      0.58       431\n",
      "     romance       0.15      0.03      0.04       151\n",
      "   adventure       0.26      0.19      0.22       139\n",
      "    thriller       0.24      0.15      0.18       309\n",
      "       crime       0.28      0.12      0.17       107\n",
      "       adult       0.46      0.38      0.41       112\n",
      "        news       0.25      0.12      0.16        34\n",
      "     musical       0.19      0.10      0.13        50\n",
      "      family       0.25      0.17      0.20       150\n",
      "   biography       0.00      0.00      0.00        61\n",
      "   game-show       0.70      0.47      0.57        40\n",
      "   animation       0.26      0.13      0.18       104\n",
      "       short       0.35      0.38      0.36      1045\n",
      "         war       0.38      0.15      0.21        20\n",
      "  reality-tv       0.38      0.28      0.32       192\n",
      "      action       0.34      0.25      0.29       263\n",
      " documentary       0.71      0.75      0.73      2659\n",
      "      comedy       0.52      0.49      0.50      1443\n",
      "      sci-fi       0.41      0.29      0.34       143\n",
      "   talk-show       0.40      0.21      0.27        81\n",
      "     mystery       0.25      0.11      0.15        56\n",
      "     history       0.00      0.00      0.00        45\n",
      "       drama       0.54      0.70      0.61      2697\n",
      "       sport       0.48      0.32      0.38        93\n",
      "     fantasy       0.17      0.07      0.10        74\n",
      "       music       0.48      0.47      0.47       144\n",
      "     western       0.86      0.78      0.82       200\n",
      "\n",
      "    accuracy                           0.54     10843\n",
      "   macro avg       0.36      0.29      0.31     10843\n",
      "weighted avg       0.52      0.54      0.52     10843\n"
     ]
    }
   ],
   "id": "36a154990071fc8f"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T23:05:05.212827100Z",
     "start_time": "2023-11-30T23:05:05.197199200Z"
    }
   },
   "execution_count": 27,
   "outputs": [],
   "id": "4baf680509af6e74"
  }
 ]
}
